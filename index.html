<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> OGAS</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">OGAS</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['万某的博客', '欢迎', 'Приятно с вами познакомиться！'],
        startDelay: 0,
        typeSpeed: 50,
        loop: true,
        backSpeed: 50,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  
<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content">欢迎来到我的个人博客，技术力不足见谅！使用电脑网页打开以获得最佳效果（</div>
</div>


<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-浅扩散模型-DDSP-SVC"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/10/02/%E6%B5%85%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B-DDSP-SVC/"
    >浅扩散模型-DDSP-SVC</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/10/02/%E6%B5%85%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B-DDSP-SVC/" class="article-date">
  <time datetime="2023-10-02T13:36:25.000Z" itemprop="datePublished">2023-10-02</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>基于 DDSP（可微分数字信号处理）的实时端到端歌声转换系统</p>
<p>（3.0 - 实验性）浅扩散模型 （DDSP + Diff-SVC 重构版）<br><img src="/2023/10/02/%E6%B5%85%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B-DDSP-SVC/diagram.png" alt="diagram.png"><br>数据准备，配置编码器（hubert 或者 contentvec ) 与声码器 (nsf-hifigan) 的环节与训练纯 DDSP 模型相同。</p>
<p>因为扩散模型更难训练，我们提供了一些预训练模型：</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/blob/main/hubertsoft_pitch_410k/model_0.pt">https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/blob/main/hubertsoft_pitch_410k/model_0.pt</a> (使用 ‘hubertsoft’ 编码器)</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/blob/main/pitch_400k/model_0.pt">https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/blob/main/pitch_400k/model_0.pt</a> (使用 ‘contentvec768l12’ 编码器)</p>
<p>将名为<code>model_0.pt</code>的预训练模型, 放到<code>diffusion.yaml</code>里面 “expdir: exp&#x2F;*****” 参数指定的模型导出文件夹内, 没有就新建一个, 程序会自动加载该文件夹下的预训练模型。</p>
<p>（1）预处理：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python preprocess.py -c configs/diffusion.yaml</span><br></pre></td></tr></table></figure>
<p>这个预处理也能用来训练 DDSP 模型，不用预处理两遍（但需要保证 yaml 里面的 data 下面的参数均一致）</p>
<p>（2）训练扩散模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_diff.py -c configs/diffusion.yaml</span><br></pre></td></tr></table></figure>
<p>（3）训练 DDSP 模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py -c configs/combsub.yaml</span><br></pre></td></tr></table></figure>
<p>如上所述，可以不需要重新预处理，但请检查 combsub.yaml 与 diffusion.yaml 是否参数匹配。说话人数 n_spk 可以不一致，但是尽量用相同的编号表示相同的说话人（推理更简单）。</p>
<p>（4）非实时推理：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main_diff.py -i &lt;input.wav&gt; -ddsp &lt;ddsp_ckpt.pt&gt; -diff &lt;diff_ckpt.pt&gt; -o &lt;output.wav&gt; -k &lt;keychange (semitones)&gt; -<span class="built_in">id</span> &lt;speaker_id&gt; -diffid &lt;diffusion_speaker_id&gt; -speedup &lt;speedup&gt; -method &lt;method&gt; -kstep &lt;kstep&gt;</span><br></pre></td></tr></table></figure>
<p>speedup 为加速倍速，method 为 pndm 或者 dpm-solver, kstep为浅扩散步数，diffid 为扩散模型的说话人id，其他参数与 main.py 含义相同。</p>
<p>如果训练时已经用相同的编号表示相同的说话人，则 -diffid 可以为空，否则需要指定 -diffid 选项。</p>
<p>如果 -ddsp 为空，则使用纯扩散模型 ，此时以输入源的 mel 进行浅扩散，若进一步 -kstep 为空，则进行完整深度的高斯扩散。</p>
<p>程序会自动检查 DDSP 模型和扩散模型的参数是否匹配 （采样率，帧长和编码器），不匹配会忽略加载 DDSP 模型并进入高斯扩散模式。</p>
<p>（5）实时 GUI :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python gui_diff.py</span><br></pre></td></tr></table></figure>

<h2 id="0-简介"><a href="#0-简介" class="headerlink" title="0.简介"></a>0.简介</h2><p>DDSP-SVC 是一个新的开源歌声转换项目，致力于开发可以在个人电脑上普及的自由 AI 变声器软件。</p>
<p>相比于比较著名的 <a target="_blank" rel="noopener" href="https://github.com/prophesier/diff-svc">Diff-SVC</a> 和 <a target="_blank" rel="noopener" href="https://github.com/svc-develop-team/so-vits-svc">SO-VITS-SVC</a>, 它训练和合成对电脑硬件的要求要低的多，并且训练时长有数量级的缩短。另外在进行实时变声时，本项目的硬件资源显著低于 SO-VITS-SVC，而 Diff-SVC 合成太慢几乎无法进行实时变声。</p>
<p>虽然 DDSP 的原始合成质量不是很理想（训练时在 tensorboard 中可以听到原始输出），但在使用基于预训练声码器的增强器增强音质后，对于部分数据集可以达到接近 SOVITS-SVC 的合成质量。</p>
<p>如果训练数据的质量非常高，可能仍然 Diff-SVC 将拥有最高的合成质量。在<code>samples</code>文件夹中包含合成示例，相关模型检查点可以从仓库发布页面下载。</p>
<p>免责声明：请确保仅使用<strong>合法获得的授权数据</strong>训练 DDSP-SVC 模型，不要将这些模型及其合成的任何音频用于非法目的。 本库作者不对因使用这些模型检查点和音频而造成的任何侵权，诈骗等违法行为负责。</p>
<p>1.1 更新：支持多说话人和音色混合。</p>
<p>2.0 更新：开始支持实时 vst 插件，并优化了 combsub 模型， 训练速度极大提升。旧的 combsub 模型仍然兼容，可用 combsub-old.yaml 训练，sins 模型不受影响，但由于训练速度远慢于 combsub, 目前版本已经不推荐使用。</p>
<h2 id="1-安装依赖"><a href="#1-安装依赖" class="headerlink" title="1. 安装依赖"></a>1. 安装依赖</h2><ol>
<li><p>安装PyTorch：我们推荐从 <a target="_blank" rel="noopener" href="https://pytorch.org/">**PyTorch 官方网站 **</a> 下载 PyTorch.</p>
</li>
<li><p>安装依赖</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt </span><br></pre></td></tr></table></figure>
<p>注： 仅在 python 3.8 (windows) + torch 1.9.1 + torchaudio 0.6.0 测试过代码，太旧或太新的依赖可能会报错。</p>
</li>
</ol>
<p>更新：python 3.8 (windows) + cuda 11.8 + torch 2.0.0 + torchaudio 2.0.1 可以运行，训练速度更快了。</p>
<h2 id="2-配置预训练模型"><a href="#2-配置预训练模型" class="headerlink" title="2. 配置预训练模型"></a>2. 配置预训练模型</h2><ul>
<li><strong>(必要操作)</strong> 下载预训练 <a target="_blank" rel="noopener" href="https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt"><strong>HubertSoft</strong></a> 编码器并将其放到 <code>pretrain/hubert</code> 文件夹。<ul>
<li>更新：现在支持 ContentVec 编码器了。你可以下载预训练 <a target="_blank" rel="noopener" href="https://ibm.ent.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr">ContentVec</a> 编码器替代 HubertSoft 编码器并修改配置文件以使用它。</li>
</ul>
</li>
<li>从 <a target="_blank" rel="noopener" href="https://openvpi.github.io/vocoders">DiffSinger 社区声码器项目</a> 下载基于预训练声码器的增强器，并解压至 <code>pretrain/</code> 文件夹。<ul>
<li>注意：你应当下载名称中带有<code>nsf_hifigan</code>的压缩文件，而非<code>nsf_hifigan_finetune</code>。</li>
</ul>
</li>
</ul>
<h2 id="3-预处理"><a href="#3-预处理" class="headerlink" title="3. 预处理"></a>3. 预处理</h2><h3 id="1-配置训练数据集和验证数据集"><a href="#1-配置训练数据集和验证数据集" class="headerlink" title="1. 配置训练数据集和验证数据集"></a>1. 配置训练数据集和验证数据集</h3><h4 id="1-1-手动配置："><a href="#1-1-手动配置：" class="headerlink" title="1.1 手动配置："></a>1.1 手动配置：</h4><p>将所有的训练集数据 (.wav 格式音频切片) 放到 <code>data/train/audio</code>。</p>
<p>将所有的验证集数据 (.wav 格式音频切片) 放到 <code>data/val/audio</code>。</p>
<h4 id="1-2-程序随机选择："><a href="#1-2-程序随机选择：" class="headerlink" title="1.2 程序随机选择："></a>1.2 程序随机选择：</h4><p>运行<code>python draw.py</code>,程序将帮助你挑选验证集数据（可以调整 <code>draw.py</code> 中的参数修改抽取文件的数量等参数）。</p>
<h4 id="1-3文件夹结构目录展示："><a href="#1-3文件夹结构目录展示：" class="headerlink" title="1.3文件夹结构目录展示："></a>1.3文件夹结构目录展示：</h4><ul>
<li>单人物目录结构：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├─ train</span><br><span class="line">│    ├─ audio</span><br><span class="line">│    │    ├─ aaa.wav</span><br><span class="line">│    │    ├─ bbb.wav</span><br><span class="line">│    │    └─ ....wav</span><br><span class="line">│    └─ val</span><br><span class="line">│    │    ├─ eee.wav</span><br><span class="line">│    │    ├─ fff.wav</span><br><span class="line">│    │    └─ ....wav</span><br></pre></td></tr></table></figure>
<ul>
<li>多人物目录结构：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├─ train</span><br><span class="line">│    ├─ audio</span><br><span class="line">│    │    ├─ 1</span><br><span class="line">│    │    │   ├─ aaa.wav</span><br><span class="line">│    │    │   ├─ bbb.wav</span><br><span class="line">│    │    │   └─ ....wav</span><br><span class="line">│    │    ├─ 2</span><br><span class="line">│    │    │   ├─ ccc.wav</span><br><span class="line">│    │    │   ├─ ddd.wav</span><br><span class="line">│    │    │   └─ ....wav</span><br><span class="line">│    │    └─ ...</span><br><span class="line">│    └─ val</span><br><span class="line">│    │    ├─ 1</span><br><span class="line">│    │    │   ├─ eee.wav</span><br><span class="line">│    │    │   ├─ fff.wav</span><br><span class="line">│    │    │   └─ ....wav</span><br><span class="line">│    │    ├─ 2</span><br><span class="line">│    │    │   ├─ ggg.wav</span><br><span class="line">│    │    │   ├─ hhh.wav</span><br><span class="line">│    │    │   └─ ....wav</span><br><span class="line">│    │    └─ ...</span><br></pre></td></tr></table></figure>
<h3 id="2-样例合成器模型训练"><a href="#2-样例合成器模型训练" class="headerlink" title="2. 样例合成器模型训练"></a>2. 样例合成器模型训练</h3><ol>
<li>训练基于梳齿波减法合成器的模型 (<strong>推荐</strong>)：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python preprocess.py -c configs/combsub.yaml</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>训练基于正弦波加法合成器的模型：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python preprocess.py -c configs/sins.yaml</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>您可以在预处理之前修改配置文件 <code>config/&lt;model_name&gt;.yaml</code>，默认配置适用于GTX-1660 显卡训练 44.1khz 高采样率合成器。</li>
</ol>
<h3 id="3-备注："><a href="#3-备注：" class="headerlink" title="3. 备注："></a>3. 备注：</h3><ol>
<li><p>请保持所有音频切片的采样率与 yaml 配置文件中的采样率一致！如果不一致，程序可以跑，但训练过程中的重新采样将非常缓慢。（可选：使用Adobe Audition™的响度匹配功能可以一次性完成重采样修改声道和响度匹配。）</p>
</li>
<li><p>训练数据集的音频切片总数建议为约 1000 个，另外长音频切成小段可以加快训练速度，但所有音频切片的时长不应少于 2 秒。如果音频切片太多，则需要较大的内存，配置文件中将 <code>cache_all_data</code> 选项设置为 false 可以解决此问题。</p>
</li>
<li><p>验证集的音频切片总数建议为 10 个左右，不要放太多，不然验证过程会很慢。</p>
</li>
<li><p>如果您的数据集质量不是很高，请在配置文件中将 ‘f0_extractor’ 设为 ‘crepe’。crepe 算法的抗噪性最好，但代价是会极大增加数据预处理所需的时间。</p>
</li>
<li><p>配置文件中的 ‘n_spk’ 参数将控制是否训练多说话人模型。如果您要训练<strong>多说话人</strong>模型，为了对说话人进行编号，所有音频文件夹的名称必须是<strong>不大于 ‘n_spk’ 的正整数</strong>。</p>
</li>
</ol>
<h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><h3 id="1-不使用预训练数据进行训练："><a href="#1-不使用预训练数据进行训练：" class="headerlink" title="1. 不使用预训练数据进行训练："></a>1. 不使用预训练数据进行训练：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以训练 combsub 模型为例 </span></span><br><span class="line">python train.py -c configs/combsub.yaml</span><br></pre></td></tr></table></figure>
<ol>
<li><p>训练其他模型方法类似。</p>
</li>
<li><p>可以随时中止训练，然后运行相同的命令来继续训练。</p>
</li>
<li><p>微调 (finetune)：在中止训练后，重新预处理新数据集或更改训练参数（batchsize、lr等），然后运行相同的命令。</p>
</li>
</ol>
<h3 id="2-使用预训练数据（底模）进行训练："><a href="#2-使用预训练数据（底模）进行训练：" class="headerlink" title="2. 使用预训练数据（底模）进行训练："></a>2. 使用预训练数据（底模）进行训练：</h3><ol>
<li><strong>使用预训练模型请修改配置文件中的 ‘n_spk’ 参数为 ‘2’ ,同时配置<code>train</code>目录结构为多人物目录，不论你是否训练多说话人模型。</strong></li>
<li><strong>如果你要训练一个更多说话人的模型，就不要下载预训练模型了。</strong></li>
<li>欢迎PR训练的多人底模 (请使用授权同意开源的数据集进行训练)。</li>
<li>从<a target="_blank" rel="noopener" href="https://github.com/yxlllc/DDSP-SVC/releases/download/2.0/opencpop+kiritan.zip"><strong>这里</strong></a>下载预训练模型，并将<code>model_300000.pt</code>解压到<code>.\exp\combsub-test\</code>中</li>
<li>同不使用预训练数据进行训练一样，启动训练。</li>
</ol>
<h2 id="5-可视化"><a href="#5-可视化" class="headerlink" title="5. 可视化"></a>5. 可视化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用tensorboard检查训练状态</span></span><br><span class="line">tensorboard --logdir=exp</span><br></pre></td></tr></table></figure>
<p>第一次验证 (validation) 后，在 TensorBoard 中可以看到合成后的测试音频。</p>
<p>注：TensorBoard 中的测试音频是 DDSP-SVC 模型的原始输出，并未通过增强器增强。 如果想测试模型使用增强器的合成效果（可能具有更高的合成质量），请使用下一章中描述的方法。</p>
<h2 id="6-非实时变声"><a href="#6-非实时变声" class="headerlink" title="6. 非实时变声"></a>6. 非实时变声</h2><ol>
<li>（<strong>推荐</strong>）使用预训练声码器增强 DDSP 的输出结果：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认 enhancer_adaptive_key = 0 正常音域范围内将有更高的音质</span></span><br><span class="line"><span class="comment"># 设置 enhancer_adaptive_key &gt; 0 可将增强器适配于更高的音域</span></span><br><span class="line">python main.py -i &lt;input.wav&gt; -m &lt;model_file.pt&gt; -o &lt;output.wav&gt; -k &lt;keychange (semitones)&gt; -<span class="built_in">id</span> &lt;speaker_id&gt; -e <span class="literal">true</span> -eak &lt;enhancer_adaptive_key (semitones)&gt;</span><br></pre></td></tr></table></figure></li>
<li>DDSP 的原始输出结果：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 速度快，但音质相对较低（像您在tensorboard里听到的那样）</span></span><br><span class="line">python main.py -i &lt;input.wav&gt; -m &lt;model_file.pt&gt; -o &lt;output.wav&gt; -k &lt;keychange (semitones)&gt; -e <span class="literal">false</span> -<span class="built_in">id</span> &lt;speaker_id&gt;</span><br></pre></td></tr></table></figure></li>
<li>关于 f0 提取器、响应阈值及其他参数，参见:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py -h</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>如果要使用混合说话人（捏音色）功能，增添 “-mix” 选项来设计音色，下面是个例子：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将1号说话人和2号说话人的音色按照0.5:0.5的比例混合</span></span><br><span class="line">python main.py -i &lt;input.wav&gt; -m &lt;model_file.pt&gt; -o &lt;output.wav&gt; -k &lt;keychange (semitones)&gt; -mix <span class="string">&quot;&#123;1:0.5, 2:0.5&#125;&quot;</span> -e <span class="literal">true</span> -eak 0</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="7-实时变声"><a href="#7-实时变声" class="headerlink" title="7. 实时变声"></a>7. 实时变声</h2><p>用以下命令启动简易操作界面:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python gui.py</span><br></pre></td></tr></table></figure>
<p>该前端使用了滑动窗口，交叉淡化，基于SOLA 的拼接和上下文语义参考等技术，在低延迟和资源占用的情况下可以达到接近非实时合成的音质。</p>
<p>更新：现在加入了基于相位声码器的衔接算法，但是大多数情况下 SOLA 算法已经具有足够高的拼接音质，所以它默认是关闭状态。如果您追求极端的低延迟实时变声音质，可以考虑开启它并仔细调参，有概率音质更高。但大量测试发现，如果交叉淡化时长大于0.1秒，相位声码器反而会造成音质明显劣化。</p>
<h2 id="8-感谢"><a href="#8-感谢" class="headerlink" title="8. 感谢"></a>8. 感谢</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/magenta/ddsp">ddsp</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/yxlllc/pc-ddsp">pc-ddsp</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/bshall/soft-vc">soft-vc</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/openvpi/DiffSinger">DiffSinger (OpenVPI version)</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-cn_README"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/10/02/cn_README/"
    >浅扩散模型 （DDSP + Diff-SVC 重构版）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/10/02/cn_README/" class="article-date">
  <time datetime="2023-10-02T13:26:43.000Z" itemprop="datePublished">2023-10-02</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>基于 DDSP（可微分数字信号处理）的实时端到端歌声转换系统</p>
<p>（3.0 - 实验性）浅扩散模型 （DDSP + Diff-SVC 重构版）</p>
<p><img src="/2023/10/02/cn_README/diagram.png" alt="diagram.png"><br>数据准备，配置编码器（hubert 或者 contentvec ) 与声码器 (nsf-hifigan) 的环节与训练纯 DDSP 模型相同。</p>
<p>因为扩散模型更难训练，我们提供了一些预训练模型：</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/blob/main/hubertsoft_pitch_410k/model_0.pt">https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/blob/main/hubertsoft_pitch_410k/model_0.pt</a> (使用 ‘hubertsoft’ 编码器)</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/blob/main/pitch_400k/model_0.pt">https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/blob/main/pitch_400k/model_0.pt</a> (使用 ‘contentvec768l12’ 编码器)</p>
<p>将名为<code>model_0.pt</code>的预训练模型, 放到<code>diffusion.yaml</code>里面 “expdir: exp&#x2F;*****” 参数指定的模型导出文件夹内, 没有就新建一个, 程序会自动加载该文件夹下的预训练模型。</p>
<p>（1）预处理：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python preprocess.py -c configs/diffusion.yaml</span><br></pre></td></tr></table></figure>
<p>这个预处理也能用来训练 DDSP 模型，不用预处理两遍（但需要保证 yaml 里面的 data 下面的参数均一致）</p>
<p>（2）训练扩散模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_diff.py -c configs/diffusion.yaml</span><br></pre></td></tr></table></figure>
<p>（3）训练 DDSP 模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py -c configs/combsub.yaml</span><br></pre></td></tr></table></figure>
<p>如上所述，可以不需要重新预处理，但请检查 combsub.yaml 与 diffusion.yaml 是否参数匹配。说话人数 n_spk 可以不一致，但是尽量用相同的编号表示相同的说话人（推理更简单）。</p>
<p>（4）非实时推理：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main_diff.py -i &lt;input.wav&gt; -ddsp &lt;ddsp_ckpt.pt&gt; -diff &lt;diff_ckpt.pt&gt; -o &lt;output.wav&gt; -k &lt;keychange (semitones)&gt; -<span class="built_in">id</span> &lt;speaker_id&gt; -diffid &lt;diffusion_speaker_id&gt; -speedup &lt;speedup&gt; -method &lt;method&gt; -kstep &lt;kstep&gt;</span><br></pre></td></tr></table></figure>
<p>speedup 为加速倍速，method 为 pndm 或者 dpm-solver, kstep为浅扩散步数，diffid 为扩散模型的说话人id，其他参数与 main.py 含义相同。</p>
<p>如果训练时已经用相同的编号表示相同的说话人，则 -diffid 可以为空，否则需要指定 -diffid 选项。</p>
<p>如果 -ddsp 为空，则使用纯扩散模型 ，此时以输入源的 mel 进行浅扩散，若进一步 -kstep 为空，则进行完整深度的高斯扩散。</p>
<p>程序会自动检查 DDSP 模型和扩散模型的参数是否匹配 （采样率，帧长和编码器），不匹配会忽略加载 DDSP 模型并进入高斯扩散模式。</p>
<p>（5）实时 GUI :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python gui_diff.py</span><br></pre></td></tr></table></figure>

<h2 id="0-简介"><a href="#0-简介" class="headerlink" title="0.简介"></a>0.简介</h2><p>DDSP-SVC 是一个新的开源歌声转换项目，致力于开发可以在个人电脑上普及的自由 AI 变声器软件。</p>
<p>相比于比较著名的 <a target="_blank" rel="noopener" href="https://github.com/prophesier/diff-svc">Diff-SVC</a> 和 <a target="_blank" rel="noopener" href="https://github.com/svc-develop-team/so-vits-svc">SO-VITS-SVC</a>, 它训练和合成对电脑硬件的要求要低的多，并且训练时长有数量级的缩短。另外在进行实时变声时，本项目的硬件资源显著低于 SO-VITS-SVC，而 Diff-SVC 合成太慢几乎无法进行实时变声。</p>
<p>虽然 DDSP 的原始合成质量不是很理想（训练时在 tensorboard 中可以听到原始输出），但在使用基于预训练声码器的增强器增强音质后，对于部分数据集可以达到接近 SOVITS-SVC 的合成质量。</p>
<p>如果训练数据的质量非常高，可能仍然 Diff-SVC 将拥有最高的合成质量。在<code>samples</code>文件夹中包含合成示例，相关模型检查点可以从仓库发布页面下载。</p>
<p>免责声明：请确保仅使用<strong>合法获得的授权数据</strong>训练 DDSP-SVC 模型，不要将这些模型及其合成的任何音频用于非法目的。 本库作者不对因使用这些模型检查点和音频而造成的任何侵权，诈骗等违法行为负责。</p>
<p>1.1 更新：支持多说话人和音色混合。</p>
<p>2.0 更新：开始支持实时 vst 插件，并优化了 combsub 模型， 训练速度极大提升。旧的 combsub 模型仍然兼容，可用 combsub-old.yaml 训练，sins 模型不受影响，但由于训练速度远慢于 combsub, 目前版本已经不推荐使用。</p>
<h2 id="1-安装依赖"><a href="#1-安装依赖" class="headerlink" title="1. 安装依赖"></a>1. 安装依赖</h2><ol>
<li><p>安装PyTorch：我们推荐从 <a target="_blank" rel="noopener" href="https://pytorch.org/">**PyTorch 官方网站 **</a> 下载 PyTorch.</p>
</li>
<li><p>安装依赖</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt </span><br></pre></td></tr></table></figure>
<p>注： 仅在 python 3.8 (windows) + torch 1.9.1 + torchaudio 0.6.0 测试过代码，太旧或太新的依赖可能会报错。</p>
</li>
</ol>
<p>更新：python 3.8 (windows) + cuda 11.8 + torch 2.0.0 + torchaudio 2.0.1 可以运行，训练速度更快了。</p>
<h2 id="2-配置预训练模型"><a href="#2-配置预训练模型" class="headerlink" title="2. 配置预训练模型"></a>2. 配置预训练模型</h2><ul>
<li><strong>(必要操作)</strong> 下载预训练 <a target="_blank" rel="noopener" href="https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt"><strong>HubertSoft</strong></a> 编码器并将其放到 <code>pretrain/hubert</code> 文件夹。<ul>
<li>更新：现在支持 ContentVec 编码器了。你可以下载预训练 <a target="_blank" rel="noopener" href="https://ibm.ent.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr">ContentVec</a> 编码器替代 HubertSoft 编码器并修改配置文件以使用它。</li>
</ul>
</li>
<li>从 <a target="_blank" rel="noopener" href="https://openvpi.github.io/vocoders">DiffSinger 社区声码器项目</a> 下载基于预训练声码器的增强器，并解压至 <code>pretrain/</code> 文件夹。<ul>
<li>注意：你应当下载名称中带有<code>nsf_hifigan</code>的压缩文件，而非<code>nsf_hifigan_finetune</code>。</li>
</ul>
</li>
</ul>
<h2 id="3-预处理"><a href="#3-预处理" class="headerlink" title="3. 预处理"></a>3. 预处理</h2><h3 id="1-配置训练数据集和验证数据集"><a href="#1-配置训练数据集和验证数据集" class="headerlink" title="1. 配置训练数据集和验证数据集"></a>1. 配置训练数据集和验证数据集</h3><h4 id="1-1-手动配置："><a href="#1-1-手动配置：" class="headerlink" title="1.1 手动配置："></a>1.1 手动配置：</h4><p>将所有的训练集数据 (.wav 格式音频切片) 放到 <code>data/train/audio</code>。</p>
<p>将所有的验证集数据 (.wav 格式音频切片) 放到 <code>data/val/audio</code>。</p>
<h4 id="1-2-程序随机选择："><a href="#1-2-程序随机选择：" class="headerlink" title="1.2 程序随机选择："></a>1.2 程序随机选择：</h4><p>运行<code>python draw.py</code>,程序将帮助你挑选验证集数据（可以调整 <code>draw.py</code> 中的参数修改抽取文件的数量等参数）。</p>
<h4 id="1-3文件夹结构目录展示："><a href="#1-3文件夹结构目录展示：" class="headerlink" title="1.3文件夹结构目录展示："></a>1.3文件夹结构目录展示：</h4><ul>
<li>单人物目录结构：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├─ train</span><br><span class="line">│    ├─ audio</span><br><span class="line">│    │    ├─ aaa.wav</span><br><span class="line">│    │    ├─ bbb.wav</span><br><span class="line">│    │    └─ ....wav</span><br><span class="line">│    └─ val</span><br><span class="line">│    │    ├─ eee.wav</span><br><span class="line">│    │    ├─ fff.wav</span><br><span class="line">│    │    └─ ....wav</span><br></pre></td></tr></table></figure>
<ul>
<li>多人物目录结构：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├─ train</span><br><span class="line">│    ├─ audio</span><br><span class="line">│    │    ├─ 1</span><br><span class="line">│    │    │   ├─ aaa.wav</span><br><span class="line">│    │    │   ├─ bbb.wav</span><br><span class="line">│    │    │   └─ ....wav</span><br><span class="line">│    │    ├─ 2</span><br><span class="line">│    │    │   ├─ ccc.wav</span><br><span class="line">│    │    │   ├─ ddd.wav</span><br><span class="line">│    │    │   └─ ....wav</span><br><span class="line">│    │    └─ ...</span><br><span class="line">│    └─ val</span><br><span class="line">│    │    ├─ 1</span><br><span class="line">│    │    │   ├─ eee.wav</span><br><span class="line">│    │    │   ├─ fff.wav</span><br><span class="line">│    │    │   └─ ....wav</span><br><span class="line">│    │    ├─ 2</span><br><span class="line">│    │    │   ├─ ggg.wav</span><br><span class="line">│    │    │   ├─ hhh.wav</span><br><span class="line">│    │    │   └─ ....wav</span><br><span class="line">│    │    └─ ...</span><br></pre></td></tr></table></figure>
<h3 id="2-样例合成器模型训练"><a href="#2-样例合成器模型训练" class="headerlink" title="2. 样例合成器模型训练"></a>2. 样例合成器模型训练</h3><ol>
<li>训练基于梳齿波减法合成器的模型 (<strong>推荐</strong>)：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python preprocess.py -c configs/combsub.yaml</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>训练基于正弦波加法合成器的模型：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python preprocess.py -c configs/sins.yaml</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>您可以在预处理之前修改配置文件 <code>config/&lt;model_name&gt;.yaml</code>，默认配置适用于GTX-1660 显卡训练 44.1khz 高采样率合成器。</li>
</ol>
<h3 id="3-备注："><a href="#3-备注：" class="headerlink" title="3. 备注："></a>3. 备注：</h3><ol>
<li><p>请保持所有音频切片的采样率与 yaml 配置文件中的采样率一致！如果不一致，程序可以跑，但训练过程中的重新采样将非常缓慢。（可选：使用Adobe Audition™的响度匹配功能可以一次性完成重采样修改声道和响度匹配。）</p>
</li>
<li><p>训练数据集的音频切片总数建议为约 1000 个，另外长音频切成小段可以加快训练速度，但所有音频切片的时长不应少于 2 秒。如果音频切片太多，则需要较大的内存，配置文件中将 <code>cache_all_data</code> 选项设置为 false 可以解决此问题。</p>
</li>
<li><p>验证集的音频切片总数建议为 10 个左右，不要放太多，不然验证过程会很慢。</p>
</li>
<li><p>如果您的数据集质量不是很高，请在配置文件中将 ‘f0_extractor’ 设为 ‘crepe’。crepe 算法的抗噪性最好，但代价是会极大增加数据预处理所需的时间。</p>
</li>
<li><p>配置文件中的 ‘n_spk’ 参数将控制是否训练多说话人模型。如果您要训练<strong>多说话人</strong>模型，为了对说话人进行编号，所有音频文件夹的名称必须是<strong>不大于 ‘n_spk’ 的正整数</strong>。</p>
</li>
</ol>
<h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><h3 id="1-不使用预训练数据进行训练："><a href="#1-不使用预训练数据进行训练：" class="headerlink" title="1. 不使用预训练数据进行训练："></a>1. 不使用预训练数据进行训练：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以训练 combsub 模型为例 </span></span><br><span class="line">python train.py -c configs/combsub.yaml</span><br></pre></td></tr></table></figure>
<ol>
<li><p>训练其他模型方法类似。</p>
</li>
<li><p>可以随时中止训练，然后运行相同的命令来继续训练。</p>
</li>
<li><p>微调 (finetune)：在中止训练后，重新预处理新数据集或更改训练参数（batchsize、lr等），然后运行相同的命令。</p>
</li>
</ol>
<h3 id="2-使用预训练数据（底模）进行训练："><a href="#2-使用预训练数据（底模）进行训练：" class="headerlink" title="2. 使用预训练数据（底模）进行训练："></a>2. 使用预训练数据（底模）进行训练：</h3><ol>
<li><strong>使用预训练模型请修改配置文件中的 ‘n_spk’ 参数为 ‘2’ ,同时配置<code>train</code>目录结构为多人物目录，不论你是否训练多说话人模型。</strong></li>
<li><strong>如果你要训练一个更多说话人的模型，就不要下载预训练模型了。</strong></li>
<li>欢迎PR训练的多人底模 (请使用授权同意开源的数据集进行训练)。</li>
<li>从<a target="_blank" rel="noopener" href="https://github.com/yxlllc/DDSP-SVC/releases/download/2.0/opencpop+kiritan.zip"><strong>这里</strong></a>下载预训练模型，并将<code>model_300000.pt</code>解压到<code>.\exp\combsub-test\</code>中</li>
<li>同不使用预训练数据进行训练一样，启动训练。</li>
</ol>
<h2 id="5-可视化"><a href="#5-可视化" class="headerlink" title="5. 可视化"></a>5. 可视化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用tensorboard检查训练状态</span></span><br><span class="line">tensorboard --logdir=exp</span><br></pre></td></tr></table></figure>
<p>第一次验证 (validation) 后，在 TensorBoard 中可以看到合成后的测试音频。</p>
<p>注：TensorBoard 中的测试音频是 DDSP-SVC 模型的原始输出，并未通过增强器增强。 如果想测试模型使用增强器的合成效果（可能具有更高的合成质量），请使用下一章中描述的方法。</p>
<h2 id="6-非实时变声"><a href="#6-非实时变声" class="headerlink" title="6. 非实时变声"></a>6. 非实时变声</h2><ol>
<li>（<strong>推荐</strong>）使用预训练声码器增强 DDSP 的输出结果：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认 enhancer_adaptive_key = 0 正常音域范围内将有更高的音质</span></span><br><span class="line"><span class="comment"># 设置 enhancer_adaptive_key &gt; 0 可将增强器适配于更高的音域</span></span><br><span class="line">python main.py -i &lt;input.wav&gt; -m &lt;model_file.pt&gt; -o &lt;output.wav&gt; -k &lt;keychange (semitones)&gt; -<span class="built_in">id</span> &lt;speaker_id&gt; -e <span class="literal">true</span> -eak &lt;enhancer_adaptive_key (semitones)&gt;</span><br></pre></td></tr></table></figure></li>
<li>DDSP 的原始输出结果：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 速度快，但音质相对较低（像您在tensorboard里听到的那样）</span></span><br><span class="line">python main.py -i &lt;input.wav&gt; -m &lt;model_file.pt&gt; -o &lt;output.wav&gt; -k &lt;keychange (semitones)&gt; -e <span class="literal">false</span> -<span class="built_in">id</span> &lt;speaker_id&gt;</span><br></pre></td></tr></table></figure></li>
<li>关于 f0 提取器、响应阈值及其他参数，参见:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py -h</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>如果要使用混合说话人（捏音色）功能，增添 “-mix” 选项来设计音色，下面是个例子：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将1号说话人和2号说话人的音色按照0.5:0.5的比例混合</span></span><br><span class="line">python main.py -i &lt;input.wav&gt; -m &lt;model_file.pt&gt; -o &lt;output.wav&gt; -k &lt;keychange (semitones)&gt; -mix <span class="string">&quot;&#123;1:0.5, 2:0.5&#125;&quot;</span> -e <span class="literal">true</span> -eak 0</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="7-实时变声"><a href="#7-实时变声" class="headerlink" title="7. 实时变声"></a>7. 实时变声</h2><p>用以下命令启动简易操作界面:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python gui.py</span><br></pre></td></tr></table></figure>
<p>该前端使用了滑动窗口，交叉淡化，基于SOLA 的拼接和上下文语义参考等技术，在低延迟和资源占用的情况下可以达到接近非实时合成的音质。</p>
<p>更新：现在加入了基于相位声码器的衔接算法，但是大多数情况下 SOLA 算法已经具有足够高的拼接音质，所以它默认是关闭状态。如果您追求极端的低延迟实时变声音质，可以考虑开启它并仔细调参，有概率音质更高。但大量测试发现，如果交叉淡化时长大于0.1秒，相位声码器反而会造成音质明显劣化。</p>
<h2 id="8-感谢"><a href="#8-感谢" class="headerlink" title="8. 感谢"></a>8. 感谢</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/magenta/ddsp">ddsp</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/yxlllc/pc-ddsp">pc-ddsp</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/bshall/soft-vc">soft-vc</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/openvpi/DiffSinger">DiffSinger (OpenVPI version)</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-扩散模型"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/10/02/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"
    >扩散模型</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/10/02/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2023-10-02T13:26:43.000Z" itemprop="datePublished">2023-10-02</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>图像生成领域最常见生成模型有GAN和VAE，2020年，DDPM（Denoising Diffusion Probabilistic Model）被提出，被称为扩散模型（Diffusion Model），同样可用于图像生成。近年扩散模型大热，Stability AI、OpenAI、Google Brain等相继基于扩散模型提出的以文生图，图像生成视频生成等模型。</p>
<h2 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h2><p>扩散模型：和其他生成模型一样，实现从噪声（采样自简单的分布）生成目标数据样本。</p>
<p>扩散模型包括两个过程：前向过程（forward process）和反向过程（reverse process），其中前向过程又称为扩散过程（diffusion process）。无论是前向过程还是反向过程都是一个参数化的马尔可夫链（Markov chain），其中反向过程可用于生成数据样本（它的作用类似GAN中的生成器，只不过GAN生成器会有维度变化，而DDPM的反向过程没有维度变化）。</p>
<p><img src="https://pic2.zhimg.com/80/v2-1b030c7965fb49c08658016398a36d65_1440w.webp"></p>
<p>上图截取自原论文.</p>
<p>-  到  为逐步加噪过的前向程，噪声是已知的，该过程从原始图片逐步加噪至一组纯噪声。</p>
<p>-  到  为将一组随机噪声还原为输入的过程。该过程需要学习一个去噪过程，直到还原一张图片。</p>
<p><strong>前向过程</strong></p>
<p>前向过程是加噪的过程，前向过程中图像  只和上一时刻的  有关, 该过程可以视为马尔科夫过程, 满足:</p>
<p>其中不同t的  是预先定义好的逐渐衰减的，可以是Linear，Cosine等，满足  。</p>
<p>根据以上公式，可以通过重参数化采样得到。  ，</p>
<p>经过推导，可以得出  与 的关系：</p>
<p><strong>逆向过程</strong></p>
<p>逆向过程是去噪的过程，如果得到逆向过程 ，就可以通过随机噪声$  $逐步还原出一张图像。DDPM使用神经网络  拟合逆向过程  。</p>
<p> ，可以推导出:</p>
<p>DDPM论文中不计方差，通过神经网络拟合均值 ，从而得到  ,</p>
<p>因为  和  已知，只需使用神经网络拟合 </p>
<p><strong>网络结构</strong></p>
<p>论文的源代码采用Unet实现  的预测，整个训练过程其实就是在训练Unet网络的参数</p>
<p><strong>Unet职责</strong></p>
<p>无论在前向过程还是反向过程，Unet的职责都是根据当前的样本和时间t预测噪声。</p>
<h3 id="Gaussion-Diffusion职责"><a href="#Gaussion-Diffusion职责" class="headerlink" title="Gaussion Diffusion职责"></a>Gaussion Diffusion职责</h3><p>前向过程：从1到T的时间采样一个时间  ，生成一个随机噪声加到图片上，从Unet获取预测噪声，计算损失后更新Unet梯度</p>
<p>反向过程：先从正态分布随机采样和训练样本一样大小的纯噪声图片，从T-1到0逐步重复以下步骤：从  还原  。</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p><img src="https://pic3.zhimg.com/80/v2-70f1ec85b1647b70e0b67e14a662315e_1440w.webp"></p>
<h3 id="Algorithm1：Training"><a href="#Algorithm1：Training" class="headerlink" title="Algorithm1：Training"></a>Algorithm1：Training</h3><ul>
<li>从数据中抽取一个样本，</li>
<li>从1-T中随机选取一个时间t</li>
<li>将  和t传给GaussionDiffusion，GaussionDiffusion采样一个随机噪声，加到  ，形成 ，然后将 和t放入Unet，Unet根据t生成正弦位置编码和  结合，Unet预测加的这个噪声，并返回噪声，GaussionDiffusion计算该噪声和随机噪声的损失</li>
<li>将神经网络Unet预测的噪声与之前GaussionDiffusion采样的随机噪声求L2损失，计算梯度，更新权重。</li>
<li>重复以上步骤，直到网络Unet训练完成。</li>
</ul>
<p>训练步骤中每个模块的交互如下图：</p>
<p><img src="https://pic4.zhimg.com/80/v2-33081e7d50e65e1ed4e5d1f91e67728b_1440w.webp"></p>
<h3 id="Algorithm2：Sampling"><a href="#Algorithm2：Sampling" class="headerlink" title="Algorithm2：Sampling"></a>Algorithm2：Sampling</h3><ul>
<li>- 从标准正态分布采样出 </li>
<li>- 从  依次重复以下步骤:</li>
<li>- 从标准正态分布采样  ，为重参数化做准备</li>
<li>- 根据模型求出，结合  和采样得到z利用重参数化技巧，得到 </li>
<li>- 循环结束后返回</li>
</ul>
<p> </p>
<p>采样步骤中每个模块的交互如下图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-3673b2795344783503286c32f05fc7b6_1440w.webp"></p>
<h2 id="结合代码（MindSpore版本）讲解"><a href="#结合代码（MindSpore版本）讲解" class="headerlink" title="结合代码（MindSpore版本）讲解"></a>结合代码（MindSpore版本）讲解</h2><p>代码主要分为以下几块：Unet、GaussianDiffusion、 Trainer</p>
<h3 id="1-Unet"><a href="#1-Unet" class="headerlink" title="1. Unet"></a>1. Unet</h3><p>Unet网络结构如图：</p>
<p><img src="https://pic2.zhimg.com/80/v2-01dadddd5b3082dafe535d59330ae845_1440w.webp"></p>
<p><strong>1.1 正弦位置编码</strong></p>
<p>DDPM每步训练是随机采样一个时间，为了让网络知道当前处理的是一系列去噪过程中的哪一个step，我们需要将当前t编码并传入网络之中，DDPM使用的Unet是time-condition Unet。</p>
<p>类似于Transformer的positional embedding，DDPM采用正弦位置编码（Sinusoidal Positional Embeddings），既需要位置编码有界又需要两个时间步长之间的距离与句子长度无关。为了满足这两点标准，一种思路是使用有界的周期性函数，而简单的有界周期性函数很容易想到sin和cos函数。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class SinusoidalPosEmb(nn.Cell):</span><br><span class="line">    def __init__(self, dim):</span><br><span class="line">        super().__init__()</span><br><span class="line">        half_dim = dim // 2</span><br><span class="line">        emb = math.log(10000) / (half_dim - 1)</span><br><span class="line">        emb = np.exp(np.arange(half_dim) * - emb)</span><br><span class="line">        self.emb = Tensor(emb, mindspore.float32)</span><br><span class="line">        self.Concat = _get_cache_prim(ops.Concat)(-1)</span><br><span class="line"></span><br><span class="line">    def construct(self, x):</span><br><span class="line">        emb = x[:, None] * self.emb[None, :]</span><br><span class="line">        emb = self.Concat((ops.sin(emb), ops.cos(emb)))</span><br><span class="line">        return emb</span><br></pre></td></tr></table></figure>

<p>DDPM的Unet有ResidualBlock和Attention Module</p>
<p><strong>1.2 Attention</strong></p>
<p>Attention的本质是从人类视觉注意力机制中获得灵感。大致是我们视觉在感知东西的时候，一般不会是一个场景从到头看到尾每次全部都看，而往往是根据需求观察注意特定的一部分。具体可以参考博客：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35571412">TheLongGoodbye：浅谈Attention机制的理解</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class Attention(nn.Cell):</span><br><span class="line">    def __init__(self, dim, heads=4, dim_head=32):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.scale = dim_head ** -0.5</span><br><span class="line">        self.heads = heads</span><br><span class="line">        hidden_dim = dim_head * heads</span><br><span class="line"></span><br><span class="line">        self.to_qkv = _get_cache_prim(Conv2d)(dim, hidden_dim * 3, 1, pad_mode=&#x27;valid&#x27;, has_bias=False)</span><br><span class="line">        self.to_out = _get_cache_prim(Conv2d)(hidden_dim, dim, 1, pad_mode=&#x27;valid&#x27;, has_bias=True)</span><br><span class="line">self.map = ops.Map()</span><br><span class="line">        self.partial = ops.Partial()</span><br><span class="line">        self.bmm = BMM()</span><br><span class="line">        self.split = ops.Split(axis=1, output_num=3)</span><br><span class="line">        self.softmax = ops.Softmax(-1)</span><br><span class="line"></span><br><span class="line">    def construct(self, x):</span><br><span class="line">        b, c, h, w = x.shape</span><br><span class="line">        qkv = self.split(self.to_qkv(x))</span><br><span class="line">        q, k, v = self.map(self.partial(rearrange, self.heads), qkv)</span><br><span class="line">        q = q * self.scale</span><br><span class="line">        sim = self.bmm(q.swapaxes(2, 3), k)</span><br><span class="line">        attn = self.softmax(sim)</span><br><span class="line">        out = self.bmm(attn, v.swapaxes(2, 3))</span><br><span class="line">        out = out.swapaxes(-1, -2).reshape((b, -1, h, w))</span><br><span class="line">        return self.to_out(out)</span><br></pre></td></tr></table></figure>

<p><strong>1.3 Residual Block</strong></p>
<p>是ResNet的核心模块，可以防止网络退化。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Residual(nn.Cell):</span><br><span class="line">    &quot;&quot;&quot;残差块&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, fn):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.fn = fn</span><br><span class="line"></span><br><span class="line">    def construct(self, x, *args, **kwargs):</span><br><span class="line">        return self.fn(x, *args, **kwargs) + x</span><br></pre></td></tr></table></figure>

<h3 id="2-GaussianDiffusion"><a href="#2-GaussianDiffusion" class="headerlink" title="2. GaussianDiffusion"></a>2. GaussianDiffusion</h3><p>首先定义相关的概率值，与公式相对应：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">self.betas = betas</span><br><span class="line">self.alphas_cumprod = alphas_cumprod</span><br><span class="line">self.alphas_cumprod_prev = alphas_cumprod_prev</span><br><span class="line"></span><br><span class="line"># calculations for diffusion q(x_t | x_&#123;t-1&#125;) and others</span><br><span class="line">self.sqrt_alphas_cumprod = Tensor(np.sqrt(alphas_cumprod))</span><br><span class="line">self.sqrt_one_minus_alphas_cumprod = Tensor(np.sqrt(1. - alphas_cumprod))</span><br><span class="line">self.log_one_minus_alphas_cumprod = Tensor(np.log(1. - alphas_cumprod))</span><br><span class="line">self.sqrt_recip_alphas_cumprod = Tensor(np.sqrt(1. / alphas_cumprod))</span><br><span class="line">self.sqrt_recipm1_alphas_cumprod = Tensor(np.sqrt(1. / alphas_cumprod - 1))</span><br><span class="line"></span><br><span class="line">posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)</span><br><span class="line"></span><br><span class="line">self.posterior_variance = Tensor(posterior_variance)</span><br><span class="line"></span><br><span class="line">self.posterior_log_variance_clipped = Tensor(</span><br><span class="line">    np.log(np.clip(posterior_variance, 1e-20, None)))</span><br><span class="line">self.posterior_mean_coef1 = Tensor(</span><br><span class="line">    betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))</span><br><span class="line">self.posterior_mean_coef2 = Tensor(</span><br><span class="line">    (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod))</span><br><span class="line"></span><br><span class="line">p2_loss_weight = (p2_loss_weight_k + alphas_cumprod / (1 - alphas_cumprod))\</span><br><span class="line">                  ** - p2_loss_weight_gamma</span><br><span class="line">self.p2_loss_weight = Tensor(p2_loss_weight)</span><br></pre></td></tr></table></figure>

<h3 id="计算损失"><a href="#计算损失" class="headerlink" title="计算损失"></a>计算损失</h3><p>基于Unet预测出noise，使用预测noise和真实noise计算损失：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def p_losses(self, x_start, t, noise, random_cond):</span><br><span class="line">    # 生成的真实noise</span><br><span class="line">    x = self.q_sample(x_start=x_start, t=t, noise=noise)</span><br><span class="line">    # if doing self-conditioning, 50% of the time, predict x_start from current set of times</span><br><span class="line">    if self.self_condition:</span><br><span class="line">        if random_cond:</span><br><span class="line">            _, x_self_cond = self.model_predictions(x, t)</span><br><span class="line">            x_self_cond = ops.stop_gradient(x_self_cond)</span><br><span class="line">        else:</span><br><span class="line">            x_self_cond = ops.zeros_like(x)</span><br><span class="line">    else:</span><br><span class="line">        x_self_cond = ops.zeros_like(x)</span><br><span class="line">    # model_out为基于U-net预测的pred_noise，此处self.model为Unet，ddpm默认预测目标是pred_noise。</span><br><span class="line">    model_out = self.model(x, t, x_self_cond)</span><br><span class="line">    if self.objective == &#x27;pred_noise&#x27;:</span><br><span class="line">        target = noise</span><br><span class="line">    elif self.objective == &#x27;pred_x0&#x27;:</span><br><span class="line">        target = x_start</span><br><span class="line">    elif self.objective == &#x27;pred_v&#x27;:</span><br><span class="line">        v = self.predict_v(x_start, t, noise)</span><br><span class="line">        target = v</span><br><span class="line">    else:</span><br><span class="line">        target = noise</span><br><span class="line">	# 计算损失值</span><br><span class="line">    loss = self.loss_fn(model_out, target)</span><br><span class="line">    loss = loss.reshape(loss.shape[0], -1)</span><br><span class="line">    loss = loss * extract(self.p2_loss_weight, t, loss.shape)</span><br><span class="line">    return loss.mean()</span><br></pre></td></tr></table></figure>

<h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>输出x_start，也就是原始图像，当sampling_time_steps&lt; time_steps，用下方函数：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def ddim_sample(self, shape, clip_denoise=True):</span><br><span class="line">    batch = shape[0]</span><br><span class="line">    total_timesteps, sampling_timesteps, = self.num_timesteps, self.sampling_timesteps</span><br><span class="line">    eta, objective = self.ddim_sampling_eta, self.objective</span><br><span class="line"></span><br><span class="line">    # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps</span><br><span class="line">    times = np.linspace(-1, total_timesteps - 1, sampling_timesteps + 1).astype(np.int32)</span><br><span class="line">    # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]</span><br><span class="line">    times = list(reversed(times.tolist()))</span><br><span class="line">    time_pairs = list(zip(times[:-1], times[1:]))</span><br><span class="line"></span><br><span class="line">	# 采样第一次迭代，Unet输入img为随机采样</span><br><span class="line">    img = np.random.randn(*shape).astype(np.float32)</span><br><span class="line">    x_start = None</span><br><span class="line"></span><br><span class="line">    for time, time_next in tqdm(time_pairs, desc=&#x27;sampling loop time step&#x27;):</span><br><span class="line">        # time_cond = ops.fill(mindspore.int32, (batch,), time)</span><br><span class="line">        time_cond = np.full((batch,), time).astype(np.int32)</span><br><span class="line">        x_start = Tensor(x_start) if x_start is not None else x_start</span><br><span class="line">        self_cond = x_start if self.self_condition else None</span><br><span class="line">        predict_noise, x_start, *_ = self.model_predictions(Tensor(img, mindspore.float32),</span><br><span class="line">                                                            Tensor(time_cond),</span><br><span class="line">                                                            self_cond,</span><br><span class="line">                                                            clip_denoise)</span><br><span class="line">        predict_noise, x_start = predict_noise.asnumpy(), x_start.asnumpy()</span><br><span class="line">        if time_next &lt; 0:</span><br><span class="line">            img = x_start</span><br><span class="line">            continue</span><br><span class="line"></span><br><span class="line">        alpha = self.alphas_cumprod[time]</span><br><span class="line">        alpha_next = self.alphas_cumprod[time_next]</span><br><span class="line"></span><br><span class="line">        sigma = eta * np.sqrt(((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)))</span><br><span class="line">        c = np.sqrt(1 - alpha_next - sigma ** 2)</span><br><span class="line"></span><br><span class="line">        noise = np.random.randn(*img.shape)</span><br><span class="line"></span><br><span class="line">        img = x_start * np.sqrt(alpha_next) + c * predict_noise + sigma * noise</span><br><span class="line"></span><br><span class="line">    img = self.unnormalize(img)</span><br><span class="line"></span><br><span class="line">    return img</span><br></pre></td></tr></table></figure>

<h3 id="3-Trainer-训练器"><a href="#3-Trainer-训练器" class="headerlink" title="3. Trainer 训练器"></a>3. Trainer 训练器</h3><p>data_iterator中每次取出的数据集就是一个batch_size大小，每训练一个batch，self.step就会加1。</p>
<p>DDPM的trainer采用ema（指数移动平均）优化，ema不参与训练，只参与推理，比对变量直接赋值而言，移动平均得到的值在图像上更加平缓光滑，抖动性更小。具体代码参考代码仓中ema.py</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">print(&#x27;training start&#x27;)</span><br><span class="line">        with tqdm(initial=self.step, total=self.train_num_steps, disable=False) as pbar:</span><br><span class="line">            total_loss = 0.</span><br><span class="line">            for (img,) in data_iterator:</span><br><span class="line">                model.set_train()</span><br><span class="line">                # # 随机采样time向量</span><br><span class="line">                time_emb = Tensor(</span><br><span class="line">                    np.random.randint(0, num_timesteps, (img.shape[0],)).astype(np.int32))</span><br><span class="line">                noise = Tensor(np.random.randn(*img.shape), mindspore.float32)</span><br><span class="line">                # 返回损失、计算梯度、更新梯度</span><br><span class="line">                self_cond = random.random() &lt; 0.5 if self.self_condition else False</span><br><span class="line">                loss = train_step(img, time_emb, noise, self_cond)</span><br><span class="line"></span><br><span class="line">                # 损失累加</span><br><span class="line">                total_loss += float(loss.asnumpy())</span><br><span class="line"></span><br><span class="line">                self.step += 1</span><br><span class="line">                if self.step % gradient_accumulate_every == 0:</span><br><span class="line">                    # ema和model的参数同步更新</span><br><span class="line">                    self.ema.update()</span><br><span class="line">                    pbar.set_description(f&#x27;loss: &#123;total_loss:.4f&#125;&#x27;)</span><br><span class="line">                    pbar.update(1)</span><br><span class="line">                    total_loss = 0.</span><br><span class="line"></span><br><span class="line">                accumulate_step = self.step // gradient_accumulate_every</span><br><span class="line">                accumulate_remain_step = self.step % gradient_accumulate_every</span><br><span class="line">                if self.step != 0 and accumulate_step % self.save_and_sample_every == 0\</span><br><span class="line">                        and accumulate_remain_step == 0:</span><br><span class="line"></span><br><span class="line">                    self.ema.set_train(False)</span><br><span class="line">                    self.ema.synchronize()</span><br><span class="line">                    batches = num_to_groups(self.num_samples, self.batch_size)</span><br><span class="line">                    all_images_list = list(map(lambda n: self.ema.online_model.sample(batch_size=n),</span><br><span class="line">                                               batches))</span><br><span class="line">                    self.save_images(all_images_list, accumulate_step)</span><br><span class="line">self.save(accumulate_step)</span><br><span class="line">                    self.ema.desynchronize()</span><br><span class="line"></span><br><span class="line">                if self.step &gt;= gradient_accumulate_every * self.train_num_steps:</span><br><span class="line">                    break</span><br><span class="line"></span><br><span class="line">        print(&#x27;training complete&#x27;)</span><br></pre></td></tr></table></figure>

<h3 id="DDPM论文"><a href="#DDPM论文" class="headerlink" title="DDPM论文"></a>DDPM论文</h3><p>- [Denoising Diffusion Probabilistic Models](<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1505.04597">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>)</p>
<h3 id="代码链接"><a href="#代码链接" class="headerlink" title="代码链接"></a>代码链接</h3><p>昇思大模型平台：<a href="https://link.zhihu.com/?target=https://xihe.mindspore.cn/projects/drizzlezyk/DDPM">昇思大模型平台</a></p>
<p>启智：<a href="https://link.zhihu.com/?target=https://openi.pcl.ac.cn/drizzlezyk/ddpm2">ddpm2</a></p>
<p>Github：<a href="https://link.zhihu.com/?target=https://github.com/drizzlezyk/DDPM-MindSpore">GitHub - drizzlezyk&#x2F;DDPM-MindSpore</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-DDSP-SVC"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/10/02/DDSP-SVC/"
    >DDSP-SVC</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/10/02/DDSP-SVC/" class="article-date">
  <time datetime="2023-10-02T02:04:05.000Z" itemprop="datePublished">2023-10-02</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>cuda is out of memory<br>​loss 不收敛<br>​gpu限额<br>​反正搞模型时各种的炸炉，最后昨天晚上让4g显存的破显卡跑了一夜终于练完了svc模型nmmd<br><img src="/2023/10/02/DDSP-SVC/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20231002100518.jpg" alt="微信图片_20231002100518.jpg"><br><img src="/2023/10/02/DDSP-SVC/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20231002100522.jpg" alt="微信图片_20231002100522.jpg"><br><img src="/2023/10/02/DDSP-SVC/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20231002100525.jpg" alt="微信图片_20231002100525.jpg"><br><img src="/2023/10/02/DDSP-SVC/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20231002100528.jpg" alt="微信图片_20231002100528.jpg"><br><img src="/2023/10/02/DDSP-SVC/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20231002100532.jpg" alt="微信图片_20231002100532.jpg"><br><img src="/2023/10/02/DDSP-SVC/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20231002100439.jpg" alt="微信图片_20231002100439.jpg"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-cod"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/09/26/cod/"
    >使命召唤网盘资源（1-14）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/09/26/cod/" class="article-date">
  <time datetime="2023-09-26T06:57:37.000Z" itemprop="datePublished">2023-09-26</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/17fz936MVS8RnAEQsTYtr5w?pwd=2333">https://pan.baidu.com/s/17fz936MVS8RnAEQsTYtr5w?pwd=2333</a> <br>提取码:2333</p>
<p>补<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1uG9vxulS6ChWTAZh7sjPGg?pwd=iczf%5C">https://pan.baidu.com/s/1uG9vxulS6ChWTAZh7sjPGg?pwd=iczf\</a><br>提取码：iczf</p>
<p>复制这段内容打开「百度网盘」APP即可获取 <br>链接:<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1o-g9kM1nPgrNnBMf3mHYjw">https://pan.baidu.com/s/1o-g9kM1nPgrNnBMf3mHYjw</a> 提取码:7500<br><img src="/2023/09/26/cod/a7b79b955ff61ad65d995d85d718180.jpg" alt="a7b79b955ff61ad65d995d85d718180.jpg"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-直播战地惨遭上市"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/09/26/%E7%9B%B4%E6%92%AD%E6%88%98%E5%9C%B0%E6%83%A8%E9%81%AD%E4%B8%8A%E5%B8%82/"
    >直播战地的多种思路</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/09/26/%E7%9B%B4%E6%92%AD%E6%88%98%E5%9C%B0%E6%83%A8%E9%81%AD%E4%B8%8A%E5%B8%82/" class="article-date">
  <time datetime="2023-09-26T06:18:56.000Z" itemprop="datePublished">2023-09-26</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2023/09/26/%E7%9B%B4%E6%92%AD%E6%88%98%E5%9C%B0%E6%83%A8%E9%81%AD%E4%B8%8A%E5%B8%82/6.png" alt="6.png"><br><img src="/2023/09/26/%E7%9B%B4%E6%92%AD%E6%88%98%E5%9C%B0%E6%83%A8%E9%81%AD%E4%B8%8A%E5%B8%82/1.png" alt="1.png"><br><img src="/2023/09/26/%E7%9B%B4%E6%92%AD%E6%88%98%E5%9C%B0%E6%83%A8%E9%81%AD%E4%B8%8A%E5%B8%82/2.png" alt="2.png"><br><img src="/2023/09/26/%E7%9B%B4%E6%92%AD%E6%88%98%E5%9C%B0%E6%83%A8%E9%81%AD%E4%B8%8A%E5%B8%82/3.png" alt="3.png"><br><img src="/2023/09/26/%E7%9B%B4%E6%92%AD%E6%88%98%E5%9C%B0%E6%83%A8%E9%81%AD%E4%B8%8A%E5%B8%82/4.png" alt="4.png"><br><img src="/2023/09/26/%E7%9B%B4%E6%92%AD%E6%88%98%E5%9C%B0%E6%83%A8%E9%81%AD%E4%B8%8A%E5%B8%82/5.png" alt="5.png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-杂谈"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/09/25/%E6%9D%82%E8%B0%88/"
    >杂谈</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/09/25/%E6%9D%82%E8%B0%88/" class="article-date">
  <time datetime="2023-09-25T15:11:32.000Z" itemprop="datePublished">2023-09-25</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>“欢迎使用基金会人工智能，比起一般的AI，我们的理念更为先进，比起对话或回答问题，更着重于令用户理解问题的本质……”<br>“为什么我爸妈结婚的时候没邀请我参加婚礼？”<br>“您看起来有充足的空余时间，已为您申请12份数据分析表单，请您于今天下班前完成相关统计报告，祝您愉快。”!</p>
<p><img src="/2023/09/25/%E6%9D%82%E8%B0%88/1.png" alt="1.png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-CONTROL"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/09/25/CONTROL/"
    >神作</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/09/25/CONTROL/" class="article-date">
  <time datetime="2023-09-25T05:03:59.000Z" itemprop="datePublished">2023-09-25</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>游戏control，帅炸了</p>
<p><img src="/2023/09/25/CONTROL/7.png" alt="7.png"><br><img src="/2023/09/25/CONTROL/1.png" alt="1.png"><br><img src="/2023/09/25/CONTROL/2.png" alt="2.png"><br><img src="/2023/09/25/CONTROL/3.png" alt="3.png"><br><img src="/2023/09/25/CONTROL/4.png" alt="4.png"><br><img src="/2023/09/25/CONTROL/5.png" alt="5.png"><br><img src="/2023/09/25/CONTROL/6.png" alt="6.png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-世界的过客"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/09/23/%E4%B8%96%E7%95%8C%E7%9A%84%E8%BF%87%E5%AE%A2/"
    >世界的过客-维克多崔遗作</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/09/23/%E4%B8%96%E7%95%8C%E7%9A%84%E8%BF%87%E5%AE%A2/" class="article-date">
  <time datetime="2023-09-23T13:57:00.000Z" itemprop="datePublished">2023-09-23</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>《Я в мире гость(我是世界的过客)》</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=3&id=2499851809&auto=1&height=66"></iframe>

<p>Мой сон（我的梦）</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=3&id=2499910846&auto=1&height=66"></iframe>

<p>Огни（灯火）</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=3&id=2499820888&auto=1&height=66"></iframe>

<p>Солнце моё далеко （我的太阳，触手难及）</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=3&id=2499866845&auto=1&height=66"></iframe>

<p>更多<a target="_blank" rel="noopener" href="https://music.163.com/#/djradio?id=973154500&order=1&_hash=programlist&limit=100&offset=100">https://music.163.com/#/djradio?id=973154500&amp;order=1&amp;_hash=programlist&amp;limit=100&amp;offset=100</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
    <article
  id="post-SU35设计图"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/09/23/SU35%E8%AE%BE%E8%AE%A1%E5%9B%BE/"
    >SU35设计图</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/09/23/SU35%E8%AE%BE%E8%AE%A1%E5%9B%BE/" class="article-date">
  <time datetime="2023-09-23T09:22:41.000Z" itemprop="datePublished">2023-09-23</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>苏35战斗机，是苏霍伊设计局在苏-27战斗机的基础上研制的深度改进型单座双发、超机动多用途重型战斗机，在战斗机世代上属于第四代战斗机改进型号，即第四代半战斗机<img src="/2023/09/23/SU35%E8%AE%BE%E8%AE%A1%E5%9B%BE/download.jpg" alt="download.jpg"><br><img src="/2023/09/23/SU35%E8%AE%BE%E8%AE%A1%E5%9B%BE/R-C.jpg" alt="R-C.jpg"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
   
  
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2023
        <i class="ri-heart-fill heart_icon"></i> 万晨希
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="OGAS"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">个人文章</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="https://c.binjie.fun/#/chat/1694091676524">AI工具</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="https://space.bilibili.com/446805121?spm_id_from=333.1007.0.0">B站空间</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="https://www.askbox.ink/box/uu/N3LD573T?uid=fa32fffa9e9d7fda9afc07315f738736">留言</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="http://scp-wiki-cn.wikidot.com/">SCP中国分部</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="https://user.qzone.qq.com/2760913192/infocenter">QQ空间</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->
 
<script src="/js/clickBoom2.js"></script>
 
<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="86"
        src="//music.163.com/outchain/player?type=2&id=2012172936&auto=0&height=66"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
    

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/ Relative)","tagMode":false,"log":false,"model":{"jsonPath":"/null"},"display":{"position":"right","width":100,"height":180},"mobile":{"show":false},"react":{"opacityDefault":0.7}});</script></body>

</html>